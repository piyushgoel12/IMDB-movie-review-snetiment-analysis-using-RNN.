{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-q3KB5zi-B7Y"
      },
      "outputs": [],
      "source": [
        "#Here we are performing sentiment analysis on the dataset provided publibly by IMDB.\n",
        "\n",
        "#Importing all the necessary libraries. \n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now that we have imported all the necessary libraries. Now we will set a specific length of the features present in the dataset.\n",
        "max_features = 10000"
      ],
      "metadata": {
        "id": "gbCrWPv9-Wom"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will load the IMDB movie review sentiment analysis dataset. \n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKr3AgvM-z2c",
        "outputId": "7d67dd1b-0169-4575-c4c9-f0f3d80df46a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will define a fixd length to all the sequences. \n",
        "maxlen = 200\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "f5LZtGJj_ZBK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we will define the RNN model for the sentiment analysis. \n",
        "#Here we will be using the \"tanh\" activation function.\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(1, activation='tanh'))"
      ],
      "metadata": {
        "id": "4vnUGsDr_ksU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the RNN model. \n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "59sDc6xD_yXE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the RNN model.\n",
        "#Total number of epochs we're taking here are 10, and the bacth size will be 30.\n",
        "#Here we've added an embedding layer, a LSTM layer and a dense layer for better working of the model.\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=30, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEBNnF46_6D6",
        "outputId": "033251f4-b14b-48e6-c165-3beb8f0ab4c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "834/834 [==============================] - 187s 221ms/step - loss: 0.5093 - accuracy: 0.7732 - val_loss: 0.4261 - val_accuracy: 0.8246\n",
            "Epoch 2/10\n",
            "834/834 [==============================] - 183s 219ms/step - loss: 0.6597 - accuracy: 0.7580 - val_loss: 0.5370 - val_accuracy: 0.7611\n",
            "Epoch 3/10\n",
            "834/834 [==============================] - 184s 221ms/step - loss: 0.4045 - accuracy: 0.8392 - val_loss: 0.6681 - val_accuracy: 0.7754\n",
            "Epoch 4/10\n",
            "834/834 [==============================] - 181s 217ms/step - loss: 0.3255 - accuracy: 0.8835 - val_loss: 0.4814 - val_accuracy: 0.8341\n",
            "Epoch 5/10\n",
            "834/834 [==============================] - 180s 216ms/step - loss: 0.4002 - accuracy: 0.8414 - val_loss: 0.4618 - val_accuracy: 0.8162\n",
            "Epoch 6/10\n",
            "834/834 [==============================] - 181s 218ms/step - loss: 0.3074 - accuracy: 0.8880 - val_loss: 0.5249 - val_accuracy: 0.8301\n",
            "Epoch 7/10\n",
            "834/834 [==============================] - 193s 232ms/step - loss: 0.2214 - accuracy: 0.9382 - val_loss: 0.7342 - val_accuracy: 0.8372\n",
            "Epoch 8/10\n",
            "834/834 [==============================] - 190s 228ms/step - loss: 0.1613 - accuracy: 0.9591 - val_loss: 0.7092 - val_accuracy: 0.8472\n",
            "Epoch 9/10\n",
            "834/834 [==============================] - 191s 229ms/step - loss: 0.1423 - accuracy: 0.9668 - val_loss: 0.8933 - val_accuracy: 0.8453\n",
            "Epoch 10/10\n",
            "834/834 [==============================] - 194s 233ms/step - loss: 0.1140 - accuracy: 0.9761 - val_loss: 1.0519 - val_accuracy: 0.8413\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efd6a6ccd60>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#No we will evaluate the RNN model on the testing dataset and will provide the results.\n",
        "\n",
        "#Now we will evaluate the model on the testing set of the dataset and will present the test results. \n",
        "\n",
        "score, acc = model.evaluate(X_test, y_test, batch_size=30)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by9Zc4IYIgoH",
        "outputId": "57bad760-ae5a-435f-f86f-d2dfb8c622b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "834/834 [==============================] - 31s 37ms/step - loss: 1.0519 - accuracy: 0.8413\n",
            "Test score: 1.0519083738327026\n",
            "Test accuracy: 0.8413199782371521\n"
          ]
        }
      ]
    }
  ]
}